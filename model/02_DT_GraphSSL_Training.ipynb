{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9df5aa25",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5244ad3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# PyG\n",
    "from torch_geometric.nn import RGCNConv, TransformerConv, global_mean_pool\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880efe3d",
   "metadata": {},
   "source": [
    "# General Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6977444d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "OUT_BACKBONE = \"/lustre/home/dante/compartido/models/backbone_ssl.pth\"\n",
    "OUT_LOSS_PLOT = \"/lustre/home/dante/compartido/models/train_loss.png\"\n",
    "\n",
    "EPOCHS = 400\n",
    "BATCH_SIZE = 1024\n",
    "LR = 5e-5\n",
    "WEIGHT_DECAY = 1e-5\n",
    "HIDDEN_DIM = 512\n",
    "PROJ_DIM = 256\n",
    "MASK_PROB = 0.25\n",
    "EDGE_DROP_P = 0.1\n",
    "NODE_DROP_P = 0.15\n",
    "ATTR_JITTER_STD = 0.02\n",
    "TEMPERATURE = 0.1\n",
    "GRAD_CLIP = 2.0\n",
    "NUM_WORKERS = 8\n",
    "SEED = 42\n",
    "CHECKPOINT_EVERY = 10  # epochs\n",
    "\n",
    "# Feature indices (assume node features are [x,y,s,a,o,dir] in that order)\n",
    "POS_IDX = [0, 1]\n",
    "\n",
    "# Seeds\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] running on device: {device}\")\n",
    "\n",
    "# Import dataset helpers\n",
    "try:\n",
    "    # dataset_dynamic must expose: dataloader, build_graphs_from_batch\n",
    "    from dataset_dynamic import dataloader, build_graphs_from_batch\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Could not import dataloader/build_graphs_from_batch from dataset_dynamic.py: \" + str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269cc59f",
   "metadata": {},
   "source": [
    "# Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ddc4e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def clone_data(g: Data):\n",
    "    \"\"\"Deep-copy a PyG Data object (safe to mutate).\"\"\"\n",
    "    g2 = Data(\n",
    "        x=g.x.clone() if g.x is not None else None,\n",
    "        edge_index=g.edge_index.clone() if g.edge_index is not None else None,\n",
    "    )\n",
    "    for k, v in g:\n",
    "        if k in (\"x\", \"edge_index\"):\n",
    "            continue\n",
    "        g2[k] = v\n",
    "    # copy edge_type if exists\n",
    "    if hasattr(g, \"edge_type\"):\n",
    "        g2.edge_type = g.edge_type.clone()\n",
    "    if hasattr(g, \"frame_id\"):\n",
    "        g2.frame_id = g.frame_id\n",
    "    if hasattr(g, \"play_id\"):\n",
    "        g2.play_id = g.play_id\n",
    "    return g2\n",
    "\n",
    "def node_drop(g: Data, p=NODE_DROP_P):\n",
    "    n = g.num_nodes\n",
    "    if p <= 0:\n",
    "        return g\n",
    "    keep_mask = torch.rand(n, device=g.x.device) > p\n",
    "    if keep_mask.all().item() is False and keep_mask.sum().item() == 0:\n",
    "        # keep at least 1\n",
    "        keep_mask[random.randint(0, n-1)] = True\n",
    "    idx = torch.nonzero(keep_mask, as_tuple=False).view(-1)\n",
    "    mapping = -torch.ones(n, dtype=torch.long, device=g.x.device)\n",
    "    mapping[idx] = torch.arange(idx.size(0), device=g.x.device)\n",
    "    # filter edges\n",
    "    ei = g.edge_index\n",
    "    src, dst = ei\n",
    "    mask_e = keep_mask[src] & keep_mask[dst]\n",
    "    new_ei = torch.stack([mapping[src[mask_e]], mapping[dst[mask_e]]], dim=0)\n",
    "    g.x = g.x[idx]\n",
    "    g.edge_index = new_ei\n",
    "    if hasattr(g, \"edge_type\"):\n",
    "        g.edge_type = g.edge_type[mask_e]\n",
    "    return g\n",
    "\n",
    "def edge_perturb(g: Data, drop_p=EDGE_DROP_P):\n",
    "    m = g.edge_index.size(1)\n",
    "    if m == 0 or drop_p <= 0:\n",
    "        return g\n",
    "    keep = torch.rand(m, device=g.edge_index.device) > drop_p\n",
    "    g.edge_index = g.edge_index[:, keep]\n",
    "    if hasattr(g, \"edge_type\"):\n",
    "        g.edge_type = g.edge_type[keep]\n",
    "    return g\n",
    "\n",
    "def attr_mask_and_jitter(g: Data, mask_p=MASK_PROB, jitter_std=ATTR_JITTER_STD):\n",
    "    # mask some node features and jitter positions\n",
    "    mask = torch.rand_like(g.x) > mask_p\n",
    "    g.x = g.x * mask.float()\n",
    "    # jitter positions (first two cols)\n",
    "    g.x[:, POS_IDX] = g.x[:, POS_IDX] + jitter_std * torch.randn_like(g.x[:, POS_IDX])\n",
    "    return g\n",
    "\n",
    "def small_rotation_and_translate(g: Data, max_angle_deg=5.0, translate_scale=0.5):\n",
    "    # rotate around center (approx), small angle in degrees\n",
    "    angle = (random.uniform(-max_angle_deg, max_angle_deg) * math.pi / 180.0)\n",
    "    cosA, sinA = math.cos(angle), math.sin(angle)\n",
    "    x = g.x[:, POS_IDX].clone()\n",
    "    # center\n",
    "    center = x.mean(dim=0, keepdim=True)\n",
    "    x0 = x - center\n",
    "    R = torch.tensor([[cosA, -sinA],[sinA, cosA]], device=x.device)\n",
    "    xr = (x0 @ R.t()) + center\n",
    "    g.x[:, POS_IDX] = xr\n",
    "    # small translate\n",
    "    trans = (translate_scale * torch.randn(2, device=x.device))\n",
    "    g.x[:, POS_IDX] += trans\n",
    "    return g\n",
    "\n",
    "def graph_augment(g: Data):\n",
    "    g2 = clone_data(g)\n",
    "    # Apply sequence of augmentations (order matters a bit)\n",
    "    g2 = node_drop(g2, NODE_DROP_P)\n",
    "    g2 = edge_perturb(g2, EDGE_DROP_P)\n",
    "    g2 = attr_mask_and_jitter(g2, MASK_PROB, ATTR_JITTER_STD)\n",
    "    g2 = small_rotation_and_translate(g2)\n",
    "    return g2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d50f697",
   "metadata": {},
   "source": [
    "# Model: Encoder + proj + recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ec571",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class DynamicEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, num_relations=4, transformer_heads=4):\n",
    "        super().__init__()\n",
    "        # two-layer RGCN (relational)\n",
    "        self.rgcn1 = RGCNConv(in_dim, hidden_dim, num_relations)\n",
    "        self.rgcn2 = RGCNConv(hidden_dim, hidden_dim, num_relations)\n",
    "        # transformer conv for global attention\n",
    "        self.trans = TransformerConv(hidden_dim, hidden_dim // transformer_heads, heads=transformer_heads)\n",
    "        # GRUCell for per-node temporal update (we use small state per node)\n",
    "        self.gru = nn.GRUCell(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type=None, h_prev=None):\n",
    "        # x: [N, F], edge_index: [2, E]\n",
    "        h = F.relu(self.rgcn1(x, edge_index, edge_type))\n",
    "        h = F.relu(self.rgcn2(h, edge_index, edge_type))\n",
    "        # transformer expects x and edge_index; returns [N, out]\n",
    "        h = self.trans(h, edge_index)\n",
    "        # temporal update: if h_prev provided (node states), apply GRUCell per node\n",
    "        if h_prev is not None:\n",
    "            # h_prev and h must be of same nodes; assume same ordering\n",
    "            h = self.gru(h, h_prev)\n",
    "        return h  # [N, hidden_dim]\n",
    "\n",
    "DynamicGraphEncoder = DynamicEncoder\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, hidden_dim, proj_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, proj_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: [num_nodes, hidden_dim] or [B, hidden_dim] (if pooled)\n",
    "        z = self.net(x)\n",
    "        return F.normalize(z, dim=-1)\n",
    "\n",
    "class ReconstructionHead(nn.Module):\n",
    "    def __init__(self, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cff0fee",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7759e40",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def info_nce_loss(z1, z2, tau=TEMPERATURE):\n",
    "    # z1, z2: [B, D]\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "\n",
    "    logits = (z1 @ z2.T) / max(tau, 1e-6)\n",
    "    logits = torch.nan_to_num(logits, nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "\n",
    "    labels = torch.arange(z1.size(0), device=z1.device)\n",
    "    loss1 = F.cross_entropy(logits, labels)\n",
    "    loss2 = F.cross_entropy(logits.T, labels)\n",
    "    loss = 0.5 * (loss1 + loss2)\n",
    "\n",
    "    if not torch.isfinite(loss):\n",
    "        print(\"[WARN] NaN detected in InfoNCE, replacing with 0\")\n",
    "        loss = torch.tensor(0.0, device=z1.device)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_recon_loss(pred, target, node_mask):\n",
    "    # pred: [N, F], target: [N, F], node_mask: boolean [N] selects nodes to compute loss\n",
    "    if node_mask is None:\n",
    "        return F.mse_loss(pred, target)\n",
    "    if node_mask.sum() == 0:\n",
    "        return torch.tensor(0.0, device=pred.device)\n",
    "    return F.mse_loss(pred[node_mask], target[node_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15540cc",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bed2b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Ajusta a tus features\n",
    "IN_DIM = 6      # x, y, s, a, o, dir\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 6\n",
    "PROJ_DIM = 128\n",
    "\n",
    "encoder = DynamicGraphEncoder(IN_DIM, HIDDEN_DIM).to(device)\n",
    "projection = ProjectionHead(HIDDEN_DIM, PROJ_DIM).to(device)\n",
    "reconstruction = ReconstructionHead(HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + \n",
    "    list(projection.parameters()) + \n",
    "    list(reconstruction.parameters()),\n",
    "    lr=1e-4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b452030f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32723b95",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(encoder, proj_head, recon_head, dataloader,\n",
    "               optimizer, scaler, scheduler=None, epochs=EPOCHS, device=device):\n",
    "\n",
    "    encoder.train()\n",
    "    proj_head.train()\n",
    "    recon_head.train()\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0.0\n",
    "        n_steps = 0\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\", ncols=120)\n",
    "        for batch in pbar:\n",
    "            # batch expected from dataset_dynamic.dataloader:\n",
    "            # either lists of Data objects per batch, or tensors to convert via build_graphs_from_batch\n",
    "            if batch[0] is None:\n",
    "                continue\n",
    "            X_t_batch, X_tp1_batch = batch  # these are tensors or already preprocessed\n",
    "            graphs_t = build_graphs_from_batch(X_t_batch)\n",
    "            graphs_tp1 = build_graphs_from_batch(X_tp1_batch)\n",
    "\n",
    "            # process per sample in batch (we can vectorize later if needed)\n",
    "            batch_losses = []\n",
    "            optimizer.zero_grad()\n",
    "            for g_t, g_tp1 in zip(graphs_t, graphs_tp1):\n",
    "                # ensure on CPU->GPU transfer\n",
    "                g_t = g_t.to(device)\n",
    "                g_tp1 = g_tp1.to(device)\n",
    "\n",
    "                # create two augmented views from g_t\n",
    "                v1 = graph_augment(g_t)\n",
    "                v2 = graph_augment(g_t)\n",
    "\n",
    "                # optional: small chance to also use g_tp1 as view to inject temporal signal\n",
    "                if random.random() < 0.1:\n",
    "                    v2 = graph_augment(g_tp1)\n",
    "\n",
    "                # node feature masks for reconstruction: choose nodes to mask (boolean per node)\n",
    "                node_mask = (torch.rand(v1.x.size(0), device=v1.x.device) < MASK_PROB)\n",
    "\n",
    "                with autocast():\n",
    "                    # get node embeddings\n",
    "                    # no previous hidden state tracked across batches — here we use g_tp1 embedding as pseudo-h_prev\n",
    "                    h_v1 = encoder(v1.x, v1.edge_index, getattr(v1, \"edge_type\", None), None)\n",
    "                    h_v2 = encoder(v2.x, v2.edge_index, getattr(v2, \"edge_type\", None), None)\n",
    "\n",
    "                    # pool graph-level embeddings (mean pool)\n",
    "                    g_emb1 = h_v1.mean(dim=0, keepdim=True)  # [1, hidden]\n",
    "                    g_emb2 = h_v2.mean(dim=0, keepdim=True)\n",
    "\n",
    "                    # projection for contrastive\n",
    "                    z1 = proj_head(g_emb1)  # [1, proj_dim]\n",
    "                    z2 = proj_head(g_emb2)\n",
    "\n",
    "                    # reconstruction predictions for masked nodes (use v1)\n",
    "                    node_preds = recon_head(h_v1)  # [N, feat_dim]\n",
    "                    recon_loss = masked_recon_loss(node_preds, v1.x, node_mask)\n",
    "\n",
    "                    # contrastive loss expects batch >1; we will accumulate small batches and compute symmetric NCE across list\n",
    "                    # For simplicity we will collect g_embs per micro-batch and compute NCE outside loop.\n",
    "                    batch_losses.append((z1, z2, recon_loss))\n",
    "\n",
    "            # Now compute contrastive over micro-batch of embeddings\n",
    "            if len(batch_losses) == 0:\n",
    "                continue\n",
    "\n",
    "            # Unzip\n",
    "            z1_list = torch.cat([t[0] for t in batch_losses], dim=0)  # [M, D]\n",
    "            z2_list = torch.cat([t[1] for t in batch_losses], dim=0)\n",
    "            recon_losses = torch.stack([t[2] for t in batch_losses], dim=0).mean()\n",
    "\n",
    "            with autocast():\n",
    "                loss_contrast = info_nce_loss(z1_list, z2_list)\n",
    "                loss_total = loss_contrast + 0.5 * recon_losses\n",
    "\n",
    "            if not torch.isfinite(loss_total):\n",
    "                print(f\"[WARN] NaN detected at epoch {epoch}, skipping batch\")\n",
    "                continue\n",
    "\n",
    "            scaler.scale(loss_total).backward()\n",
    "            # gradient clipping (unscale first)\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), GRAD_CLIP)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss_total.item()\n",
    "            n_steps += 1\n",
    "            pbar.set_postfix({\"loss\": f\"{(total_loss / n_steps):.4f}\"})\n",
    "\n",
    "        epoch_loss = total_loss / max(1, n_steps)\n",
    "        loss_history.append(epoch_loss)\n",
    "        print(f\"[Epoch {epoch}] avg loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # checkpoint\n",
    "        if epoch % CHECKPOINT_EVERY == 0:\n",
    "            # save a partial checkpoint of encoder weights\n",
    "            os.makedirs(os.path.dirname(OUT_BACKBONE), exist_ok=True)\n",
    "            torch.save(encoder.state_dict(), OUT_BACKBONE + f\".ckpt_epoch{epoch}\")\n",
    "            print(f\"[INFO] checkpoint saved: {OUT_BACKBONE}.ckpt_epoch{epoch}\")\n",
    "\n",
    "        # scheduler step (actualiza el learning rate)\n",
    "        if 'scheduler' in locals():\n",
    "            scheduler.step()\n",
    "\n",
    "    # final save\n",
    "    os.makedirs(os.path.dirname(OUT_BACKBONE), exist_ok=True)\n",
    "    torch.save(encoder.state_dict(), OUT_BACKBONE)\n",
    "    print(f\"[INFO] Final backbone saved to {OUT_BACKBONE}\")\n",
    "\n",
    "    # plot loss\n",
    "    try:\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.plot(loss_history, label=\"train_loss\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUT_LOSS_PLOT)\n",
    "        print(f\"[INFO] Loss plot saved to {OUT_LOSS_PLOT}\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Could not save loss plot:\", e)\n",
    "\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a0500",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d63f40",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # build models\n",
    "    encoder = DynamicEncoder(in_dim=6, hidden_dim=HIDDEN_DIM).to(device)\n",
    "    proj_head = ProjectionHead(HIDDEN_DIM, PROJ_DIM).to(device)\n",
    "    recon_head = ReconstructionHead(HIDDEN_DIM, out_dim=6).to(device)\n",
    "\n",
    "    params = list(encoder.parameters()) + list(proj_head.parameters()) + list(recon_head.parameters())\n",
    "    optimizer = torch.optim.AdamW(params, lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # ✅ nuevo: scheduler suave para estabilizar el entrenamiento\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=EPOCHS,\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    print(\"[INFO] Starting training...\")\n",
    "    losses = train_loop(\n",
    "        encoder, proj_head, recon_head,\n",
    "        dataloader, optimizer, scaler,\n",
    "        scheduler=scheduler,   # ✅ pasa el scheduler\n",
    "        epochs=EPOCHS, device=device\n",
    "    )\n",
    "    print(\"[INFO] Training finished.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
